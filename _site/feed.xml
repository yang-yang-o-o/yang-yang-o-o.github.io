<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-08-06T14:16:42+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Yang yang</title><entry><title type="html">一种球面均匀采样点生成方法</title><link href="http://localhost:4000/%E7%90%83%E9%9D%A2%E5%9D%87%E5%8C%80%E8%A7%86%E7%82%B9%E7%94%9F%E6%88%90/" rel="alternate" type="text/html" title="一种球面均匀采样点生成方法" /><published>2022-02-24T22:48:00+08:00</published><updated>2022-02-24T22:48:00+08:00</updated><id>http://localhost:4000/%E7%90%83%E9%9D%A2%E5%9D%87%E5%8C%80%E8%A7%86%E7%82%B9%E7%94%9F%E6%88%90</id><content type="html" xml:base="http://localhost:4000/%E7%90%83%E9%9D%A2%E5%9D%87%E5%8C%80%E8%A7%86%E7%82%B9%E7%94%9F%E6%88%90/">&lt;h3 id=&quot;1前言&quot;&gt;1.前言&lt;/h3&gt;

&lt;p&gt;在利用OpenGL渲染图像数据，或者控制机械臂末端运动时，有时会需要在一个球面上均匀的采样一些点，并且获取这些采样点相对于球心坐标系的位姿。本文介绍一种迭代生成球面均匀采样点的方法，并提供代码实现。&lt;/p&gt;

&lt;h3 id=&quot;2球面均匀采样点生成方法&quot;&gt;2.球面均匀采样点生成方法&lt;/h3&gt;

&lt;p&gt;该方法采用迭代分割多面体棱的方式来获取球面上的均匀分布点，首先采用正二十面体作为待分割的多面体，然后迭代的将每个三角形分成4个几乎全等的等边三角形，&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/segmentation.png?raw=true&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;随着迭代的进行，三角形的个数不断增加，多面体总的顶点数也不断增加，当总的顶点数大于等于设定的最大采样点数目时停止迭代，此时将所得的多面体的顶点作为球面上的均匀分布点。下图是在上半球面上均匀生成的球面点，&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/view_sphere.gif?raw=true&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;将物体置于球心，如果用于图像渲染，则可将相机放置于各个球面均匀采样点上，然后生成物体各个视角小的渲染图像，而这些渲染图像后续可用于模型训练，如果用于机械臂操作，这些球面均匀的采样点可以作为机械臂末端的途经点，从而对放置于球心的物体进行数据采样或者其他操作。&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/view_sphere_luoding.gif?raw=true&quot; /&gt;
&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h5 id=&quot;部分代码&quot;&gt;部分代码&lt;/h5&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Samples views from a sphere.
&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sample_views&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_n_views&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;azimuth_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;elev_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&apos;&apos;&apos;
    Viewpoint sampling from a view sphere.

    :param min_n_views: Minimum required number of views on the whole view sphere.
    :param radius: Radius of the view sphere.
    :param azimuth_range: Azimuth range from which the viewpoints are sampled.
    :param elev_range: Elevation range from which the viewpoints are sampled.
    :return: List of views, each represented by a 3x3 rotation matrix and
             a 3x1 translation vector.
    &apos;&apos;&apos;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Get points on a sphere
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;pts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pts_level&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hinter_sampling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_n_views&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;views&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Azimuth from (0, 2 * pi)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;azimuth&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;atan2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;azimuth&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;azimuth&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Elevation from (-0.5 * pi, 0.5 * pi)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;elev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;acos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;elev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elev&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# if hemisphere and (pt[2] &amp;lt; 0 or pt[0] &amp;lt; 0 or pt[1] &amp;lt; 0):
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;azimuth_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;azimuth&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;azimuth_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;elev_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elev_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Rotation matrix
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# The code was adopted from gluLookAt function (uses OpenGL coordinate system):
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# [1] http://stackoverflow.com/questions/5717654/glulookat-explanation
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# [2] https://www.opengl.org/wiki/GluLookAt_code
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Forward direction 相机原点指向物体原点的向量
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Up direction
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Side direction
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count_nonzero&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# f and u are parallel, i.e. we are looking along or against Z axis
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Recompute up
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt;
                      &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt;
                      &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# world to camera ，加负号使得满足右手系，z轴由物体原点指向外，
&lt;/span&gt;                                                                &lt;span class=&quot;c1&quot;&gt;# 下面绕x轴转180度后才得到真正的相机坐标系
&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Convert from OpenGL to OpenCV coordinate system
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;R_yz_flip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rotation_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R_yz_flip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# world to camera
&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Translation vector
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# pt是物体坐标系下，从物体坐标原点指向相机坐标原点的向量
&lt;/span&gt;                        &lt;span class=&quot;c1&quot;&gt;# 左乘R后得到相机下 物体坐标系原点指向相机坐标系原点的向量
&lt;/span&gt;                        &lt;span class=&quot;c1&quot;&gt;# 取负得到 相机坐标系下，相机坐标系原点指向物体坐标系原点的向量
&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;views&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;R&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;t&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;views&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pts_level&lt;/span&gt;


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h5 id=&quot;完整代码&quot;&gt;&lt;a href=&quot;https://github.com/yang-yang-o-o/tools/tree/main/view_sphere&quot;&gt;完整代码&lt;/a&gt;&lt;/h5&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3参考&quot;&gt;3.参考&lt;/h3&gt;

&lt;p&gt;Hinterstoisser et al., Simultaneous Recognition and Homography Extraction of Local Patches with a Simple Linear Classifier, BMVC 2008&lt;/p&gt;</content><author><name></name></author><category term="blog" /><category term="数据生成" /><summary type="html">1.前言</summary></entry><entry><title type="html">Texture-less object 6D pose estimation for Robotic Assembly</title><link href="http://localhost:4000/texture-less-object/" rel="alternate" type="text/html" title="Texture-less object 6D pose estimation for Robotic Assembly" /><published>2021-01-01T00:00:00+08:00</published><updated>2021-01-01T00:00:00+08:00</updated><id>http://localhost:4000/texture-less-object</id><content type="html" xml:base="http://localhost:4000/texture-less-object/">&lt;!-- ![Screenshot](https://raw.githubusercontent.com/sergiokopplin/indigo/gh-pages/assets/screen-shot.png)

Example of project - Indigo Minimalist Jekyll Template - [Demo](https://sergiokopplin.github.io/indigo/). This is a simple and minimalist template for Jekyll for those who likes to eat noodles. --&gt;
&lt;h3 id=&quot;1-video-demo&quot;&gt;1. Video Demo&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;760&quot; height=&quot;515&quot; src=&quot;https://www.youtube-nocookie.com/embed/b6RE1jMu0XI&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;h4 id=&quot;the-video-above-demonstrates-robotic-assembly-based-on-monocular-camera-where-the-assembly-clearance-between-shaft-and-hole-is-1mm&quot;&gt;  The video above demonstrates robotic assembly based on monocular camera, where the assembly clearance between shaft and hole is 1mm.&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;760&quot; height=&quot;515&quot; src=&quot;https://www.youtube-nocookie.com/embed/S8Oy4uCkzzw&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;h4 id=&quot;the-video-above-demonstrates-pose-tracking-of-mechanical-parts-where-the-red-dots-show-the-visible-edges-of-the-object&quot;&gt;  The video above demonstrates pose tracking of mechanical parts, where the red dots show the visible edges of the object.&lt;/h4&gt;

&lt;hr /&gt;

&lt;!-- What has inside?

- Gulp
- BrowserSync
- Stylus
- SVG
- No JS
- [98/100](https://developers.google.com/speed/pagespeed/insights/?url=http%3A%2F%2Fsergiokopplin.github.io%2Findigo%2F) --&gt;

&lt;h3 id=&quot;2-image-demo&quot;&gt;2. Image Demo&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/Mechanical%20Parts.png?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/112.png?raw=true&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;
&lt;/center&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 1&lt;/figcaption&gt;

&lt;h4 id=&quot;the-fig1-shows-the-pose-estimation-results-in-open-environment-of-our-method&quot;&gt;  The Fig.1 shows the pose estimation results in open environment of our method。&lt;/h4&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/2.png?raw=true&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/113.jpg?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 2&lt;/figcaption&gt;

&lt;h4 id=&quot;the-fig2-shows-the-pose-estimation-results-under-the-different-camera-pose&quot;&gt;  The Fig.2 shows the pose estimation results under the different camera pose.&lt;/h4&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/4.png?raw=true&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/4.jpg?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 3&lt;/figcaption&gt;

&lt;h4 id=&quot;the-fig3-shows-the-process-of-robot-assembly-of-our-method&quot;&gt;  The Fig.3 shows the process of robot assembly of our method.&lt;/h4&gt;

&lt;hr /&gt;

&lt;!-- [Check it out](https://sergiokopplin.github.io/indigo/) here.
If you need some help, just [tell me](https://github.com/sergiokopplin/indigo/issues). --&gt;
&lt;h3 id=&quot;3-key-inside&quot;&gt;3. Key-inside&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Yolo&lt;/li&gt;
  &lt;li&gt;Convolutional Autoencoder&lt;/li&gt;
  &lt;li&gt;Edge distance tensor&lt;/li&gt;
  &lt;li&gt;Nonlinear optimization&lt;/li&gt;
  &lt;li&gt;Opencv&lt;/li&gt;
  &lt;li&gt;Pinhole camera model&lt;/li&gt;
  &lt;li&gt;OpenGL&lt;/li&gt;
  &lt;li&gt;Rigid Object 6D pose description&lt;/li&gt;
  &lt;li&gt;Lie groups and Lie algebras&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;4-method&quot;&gt;4. Method&lt;/h3&gt;

&lt;!-- - #### Algorithm process --&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/114.png?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 4. Algorithm overview&lt;/figcaption&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/124.png?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 5. Visualization of the algorithm process&lt;/figcaption&gt;

&lt;!-- - #### Object detection

![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/115.png?raw=true)
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 6. The architecture of object detection network&lt;/figcaption&gt;

![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/118.png?raw=true)
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 7. Synthetic training data for object detection networks&lt;/figcaption&gt;

- #### Initial pose estimation

![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/116.png?raw=true)
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 8. Convolutional Auto-encoder Network&lt;/figcaption&gt;

![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/119.png?raw=true)
![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/120.png?raw=true)
![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/117.png?raw=true)

![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/121.png?raw=true)
![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/122.png?raw=true)
![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/123.png?raw=true) --&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;5-contrast&quot;&gt;5. Contrast&lt;/h3&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/125.png?raw=true&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/126.png?raw=true&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;
&lt;/center&gt;

&lt;!-- ![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/125.png?raw=true) --&gt;

&lt;!-- ![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/126.png?raw=true) --&gt;

&lt;h5 id=&quot;aae-eccv-2018-sundermeyer-m-marton-z-c-durner-m-et-al-implicit-3d-orientation-learning-for-6d-object-detection-from-rgb-imagescproceedings-of-the-european-conference-on-computer-vision-eccv-2018-699-715&quot;&gt;AAE (ECCV 2018): Sundermeyer M, Marton Z C, Durner M, et al. Implicit 3d orientation learning for 6d object detection from rgb images[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 699-715.&lt;/h5&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;6-paper&quot;&gt;6. Paper&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;面向机器人精密操作的物体六自由度单目实时位姿估计方法研究---硕士学位论文&quot;&gt;&lt;a href=&quot;https://pan.baidu.com/s/1se1wJLHhyGKLw54SS35zAQ&quot;&gt;面向机器人精密操作的物体六自由度单目实时位姿估计方法研究 - 硕士学位论文&lt;/a&gt;&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;code-come-soon&quot;&gt;code come soon.&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;</content><author><name>yang yang</name></author><category term="project" /><category term="6D pose" /><summary type="html">&amp;lt;!–</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" /><media:content medium="image" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Patch Based Real-Time 6D Object Pose Refinement Method for Robotic Manipulation</title><link href="http://localhost:4000/pose-refinement/" rel="alternate" type="text/html" title="A Patch Based Real-Time 6D Object Pose Refinement Method for Robotic Manipulation" /><published>2020-07-01T00:00:00+08:00</published><updated>2020-07-01T00:00:00+08:00</updated><id>http://localhost:4000/pose-refinement</id><content type="html" xml:base="http://localhost:4000/pose-refinement/">&lt;!-- ![Screenshot](https://raw.githubusercontent.com/sergiokopplin/indigo/gh-pages/assets/screen-shot.png)

Example of project - Indigo Minimalist Jekyll Template - [Demo](https://sergiokopplin.github.io/indigo/). This is a simple and minimalist template for Jekyll for those who likes to eat noodles. --&gt;

&lt;h3 id=&quot;1-video-demo&quot;&gt;1. Video Demo&lt;/h3&gt;
&lt;iframe src=&quot;https://player.bilibili.com/player.html?aid=689871603&amp;amp;bvid=BV1o24y1f7NQ&amp;amp;cid=883264732&amp;amp;page=1&amp;amp;high_quality=1&amp;amp;danmaku=0&quot; allowfullscreen=&quot;allowfullscreen&quot; width=&quot;100%&quot; height=&quot;500&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; sandbox=&quot;allow-top-navigation allow-same-origin allow-forms allow-scripts&quot; high_quality=&quot;1&quot;&gt;&lt;/iframe&gt;

&lt;h4 id=&quot;-link-a-patch-based-real-time-6d-object-pose-refinement-method-for-robotic-manipulation&quot;&gt;     Link: &lt;a href=&quot;https://www.bilibili.com/video/BV1o24y1f7NQ/?share_source=copy_web&amp;amp;vd_source=926e5fb00a879a3a9c35633c5af54c69&quot;&gt;A Patch Based Real-Time 6D Object Pose Refinement Method for Robotic Manipulation&lt;/a&gt;&lt;/h4&gt;

&lt;h4 id=&quot;the-above-video-shows-the-test-results-of-our-pose-refinement-method-in-the-test-environment-and-the-open-environment-where-the-initial-refined-and-ground-truth-3d-bounding-box-are-shown-in-orange-blue-and-green-respectively&quot;&gt;  The above video shows the test results of our pose refinement method in the test environment and the open environment, where the initial, refined and Ground-Truth 3D Bounding Box are shown in orange, blue and green, respectively.&lt;/h4&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;2-image-demo&quot;&gt;2. Image Demo&lt;/h3&gt;

&lt;!-- &lt;embed src=&quot;Figure_21.pdf&quot;
 type=&quot;application/pdf&quot;  width=&quot;800px&quot; height=&quot;2400px&quot;&gt;

&lt;iframe src=&quot;Figure_21.pdf&quot;
 width=&quot;800px&quot; height=&quot;2400px&quot;&gt;&lt;/iframe&gt;

&lt;object data=&quot;Figure_21.pdf&quot;
 type=&quot;application/pdf&quot; width=&quot;800px&quot; height=&quot;2400px&quot;&gt;&lt;/object&gt; --&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yang-yang-o-o/yang-yang-o-o.github.io/main/assets/images/Figure_21.jpg&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 1. Pose refinement visualization on test data.&lt;/figcaption&gt;

&lt;h4 id=&quot;the-fig1-shows-the-pose-refinement-visualization-of-our-method-where-the-refined-and-ground-truth-3d-bounding-box-are-shown-in-blue-and-green-respectively&quot;&gt;  The Fig.1 shows the Pose refinement visualization of our method, where the refined and Ground-Truth 3D Bounding Box are shown in blue and green, respectively.&lt;/h4&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/111.png?raw=true&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;
&lt;/center&gt;

&lt;!-- ![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/111.png?raw=true){: class=&quot;center-image&quot; } --&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 2. Robotic manipulation platform of Eye-in-Hand.&lt;/figcaption&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/Figure_12.jpg?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 3. The pose estimation results visualization of Eye-in-Hand.&lt;/figcaption&gt;

&lt;h4 id=&quot;as-shown-in-fig-3-we-fix-the-relative-pose-between-the-calibration-board-and-the-object-to-evaluate-the-pose-refinement-accuracy-under-various-camera-view-points&quot;&gt;  As shown in Fig. 3, we fix the relative pose between the calibration board and the object to evaluate the pose refinement accuracy under various camera view-points.&lt;/h4&gt;

&lt;hr /&gt;
&lt;!-- What has inside?

- Gulp
- BrowserSync
- Stylus
- SVG
- No JS
- [98/100](https://developers.google.com/speed/pagespeed/insights/?url=http%3A%2F%2Fsergiokopplin.github.io%2Findigo%2F) --&gt;
&lt;h3 id=&quot;3-key-inside&quot;&gt;3. Key-inside&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Siamese neural network&lt;/li&gt;
  &lt;li&gt;Image patch matching&lt;/li&gt;
  &lt;li&gt;PnP algorithm&lt;/li&gt;
  &lt;li&gt;Opencv&lt;/li&gt;
  &lt;li&gt;Pinhole camera model&lt;/li&gt;
  &lt;li&gt;Rigid Object 6D pose description&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;4-method&quot;&gt;4. Method&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/127.jpg?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;5-contrast&quot;&gt;5. Contrast&lt;/h3&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/128.png?raw=true&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/129.png?raw=true&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;
&lt;/center&gt;

&lt;h5 id=&quot;yolo6d-cvpr-2018-b-tekin-s-n-sinha-and-p-fua-real-time-seamless-single-shot-6d-object-pose-prediction-in-proceedings-of-the-ieee-conference-on-computer-vision-and-pattern-recognition-2018-pp-292301&quot;&gt;YOLO6D (CVPR 2018): B. Tekin, S. N. Sinha, and P. Fua, “Real-time seamless single shot 6d object pose prediction,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 292–301&lt;/h5&gt;

&lt;h5 id=&quot;cullnet-iccvw-2019-k-gupta-l-petersson-and-r-hartley-cullnet-calibrated-and-pose-aware-confidence-scores-for-object-pose-estimation-in-proceedings-of-the-ieeecvf-international-conference-on-computer-vision-workshops-2019-pp-00&quot;&gt;CullNet (ICCVW 2019): K. Gupta, L. Petersson, and R. Hartley, “Cullnet: Calibrated and pose aware confidence scores for object pose estimation,” in Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019, pp. 0–0&lt;/h5&gt;

&lt;h5 id=&quot;pvnet-cvpr-2019-s-peng-y-liu-q-huang-x-zhou-and-h-bao-pvnet-pixel-wise-voting-network-for-6dof-pose-estimation-in-proceedings-of-the-ieeecvf-conference-on-computer-vision-and-pattern-recognition-2019-pp-45614570&quot;&gt;PVNet (CVPR 2019): S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, “Pvnet: Pixel-wise voting network for 6dof pose estimation,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561–4570&lt;/h5&gt;

&lt;hr /&gt;
&lt;!-- [Check it out](https://sergiokopplin.github.io/indigo/) here.
If you need some help, just [tell me](https://github.com/sergiokopplin/indigo/issues). --&gt;
&lt;h3 id=&quot;6-paper&quot;&gt;6. Paper&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;a-patch-based-real-time-6d-object-pose-refinement-method-for-robotic-manipulation-password-yang&quot;&gt;&lt;a href=&quot;https://pan.baidu.com/s/1se1wJLHhyGKLw54SS35zAQ&quot;&gt;A Patch Based Real-Time 6D Object Pose Refinement Method for Robotic Manipulation&lt;/a&gt;, password: yang&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;code-come-soon&quot;&gt;code come soon.&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;</content><author><name>yang yang</name></author><category term="project" /><category term="6D pose" /><summary type="html">&amp;lt;!–</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" /><media:content medium="image" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">6D object pose estimation based on YOLO6d</title><link href="http://localhost:4000/Yolo6d/" rel="alternate" type="text/html" title="6D object pose estimation based on YOLO6d" /><published>2020-03-01T00:00:00+08:00</published><updated>2020-03-01T00:00:00+08:00</updated><id>http://localhost:4000/Yolo6d</id><content type="html" xml:base="http://localhost:4000/Yolo6d/">&lt;!-- &lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/SHocQxS0PCc&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt; --&gt;
&lt;h3 id=&quot;1-video-demo&quot;&gt;1. Video Demo&lt;/h3&gt;
&lt;iframe src=&quot;//player.bilibili.com/player.html?bvid=BV1He4y1x7du&amp;amp;page=1&amp;amp;high_quality=1&amp;amp;danmaku=0&quot; allowfullscreen=&quot;allowfullscreen&quot; width=&quot;100%&quot; height=&quot;500&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; sandbox=&quot;allow-top-navigation allow-same-origin allow-forms allow-scripts&quot;&gt;&lt;/iframe&gt;

&lt;h4 id=&quot;the-video-above-demonstrates-yolo6d-based-6d-pose-estimation-where-the-projection-of-the-3d-bbox-is-shown-in-green&quot;&gt;  The video above demonstrates yolo6d-based 6D pose estimation, where the projection of the 3D bbox is shown in green.&lt;/h4&gt;
&lt;!-- Example of project - Indigo Minimalist Jekyll Template - [Demo](https://sergiokopplin.github.io/indigo/). This is a simple and minimalist template for Jekyll for those who likes to eat noodles. --&gt;

&lt;hr /&gt;

&lt;!-- ![Screenshot](https://raw.githubusercontent.com/sergiokopplin/indigo/gh-pages/assets/screen-shot.png)

--- --&gt;

&lt;h3 id=&quot;2-key-inside&quot;&gt;2. Key-inside&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;YOLO&lt;/li&gt;
  &lt;li&gt;EPnP algorithm&lt;/li&gt;
  &lt;li&gt;Opencv&lt;/li&gt;
  &lt;li&gt;Aruco&lt;/li&gt;
  &lt;li&gt;Pinhole camera model&lt;/li&gt;
  &lt;li&gt;Rigid Object 6D pose description&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;3-reference&quot;&gt;3. Reference&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;yolo6d-paper-source-code&quot;&gt;Yolo6D: &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2018/papers/Tekin_Real-Time_Seamless_Single_CVPR_2018_paper.pdf&quot;&gt;paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/singleshotpose&quot;&gt;source code&lt;/a&gt;&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- [Check it out](https://sergiokopplin.github.io/indigo/) here.
If you need some help, just [tell me](https://github.com/sergiokopplin/indigo/issues). --&gt;

&lt;hr /&gt;</content><author><name>yang yang</name></author><category term="project" /><category term="6D pose" /><summary type="html">1. Video Demo</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" /><media:content medium="image" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>