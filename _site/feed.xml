<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-11-19T00:10:39+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Yang yang</title><entry><title type="html">Texture-less object 6D pose estimation for Robotic Assembly</title><link href="http://localhost:4000/texture-less-object/" rel="alternate" type="text/html" title="Texture-less object 6D pose estimation for Robotic Assembly" /><published>2021-01-01T00:00:00+08:00</published><updated>2021-01-01T00:00:00+08:00</updated><id>http://localhost:4000/texture-less-object</id><content type="html" xml:base="http://localhost:4000/texture-less-object/">&lt;!-- ![Screenshot](https://raw.githubusercontent.com/sergiokopplin/indigo/gh-pages/assets/screen-shot.png)

Example of project - Indigo Minimalist Jekyll Template - [Demo](https://sergiokopplin.github.io/indigo/). This is a simple and minimalist template for Jekyll for those who likes to eat noodles. --&gt;
&lt;h3 id=&quot;1-video-demo&quot;&gt;1. Video Demo&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;760&quot; height=&quot;515&quot; src=&quot;https://www.youtube-nocookie.com/embed/b6RE1jMu0XI&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;h4 id=&quot;the-video-above-demonstrates-robotic-assembly-based-on-monocular-camera-where-the-assembly-clearance-between-shaft-and-hole-is-1mm&quot;&gt;  The video above demonstrates robotic assembly based on monocular camera, where the assembly clearance between shaft and hole is 1mm.&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;760&quot; height=&quot;515&quot; src=&quot;https://www.youtube-nocookie.com/embed/S8Oy4uCkzzw&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;h4 id=&quot;the-video-above-demonstrates-pose-tracking-of-mechanical-parts-where-the-red-dots-show-the-visible-edges-of-the-object&quot;&gt;  The video above demonstrates pose tracking of mechanical parts, where the red dots show the visible edges of the object.&lt;/h4&gt;

&lt;hr /&gt;

&lt;!-- What has inside?

- Gulp
- BrowserSync
- Stylus
- SVG
- No JS
- [98/100](https://developers.google.com/speed/pagespeed/insights/?url=http%3A%2F%2Fsergiokopplin.github.io%2Findigo%2F) --&gt;

&lt;h3 id=&quot;2-image-demo&quot;&gt;2. Image Demo&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/Mechanical%20Parts.png?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/112.png?raw=true&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;
&lt;/center&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 1&lt;/figcaption&gt;

&lt;h4 id=&quot;the-fig1-shows-the-pose-estimation-results-in-open-environment-of-our-method&quot;&gt;  The Fig.1 shows the pose estimation results in open environment of our method。&lt;/h4&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/2.png?raw=true&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/113.jpg?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 2&lt;/figcaption&gt;

&lt;h4 id=&quot;the-fig2-shows-the-pose-estimation-results-under-the-different-camera-pose&quot;&gt;  The Fig.2 shows the pose estimation results under the different camera pose.&lt;/h4&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/4.png?raw=true&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/4.jpg?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 3&lt;/figcaption&gt;

&lt;h4 id=&quot;the-fig3-shows-the-process-of-robot-assembly-of-our-method&quot;&gt;  The Fig.3 shows the process of robot assembly of our method.&lt;/h4&gt;

&lt;hr /&gt;

&lt;!-- [Check it out](https://sergiokopplin.github.io/indigo/) here.
If you need some help, just [tell me](https://github.com/sergiokopplin/indigo/issues). --&gt;
&lt;h3 id=&quot;3-key-inside&quot;&gt;3. Key-inside&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Yolo&lt;/li&gt;
  &lt;li&gt;Convolutional Autoencoder&lt;/li&gt;
  &lt;li&gt;Edge distance tensor&lt;/li&gt;
  &lt;li&gt;Nonlinear optimization&lt;/li&gt;
  &lt;li&gt;Opencv&lt;/li&gt;
  &lt;li&gt;Pinhole camera model&lt;/li&gt;
  &lt;li&gt;OpenGL&lt;/li&gt;
  &lt;li&gt;Rigid Object 6D pose description&lt;/li&gt;
  &lt;li&gt;Lie groups and Lie algebras&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;4-method&quot;&gt;4. Method&lt;/h3&gt;

&lt;!-- - #### Algorithm process --&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/114.png?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 4. Algorithm overview&lt;/figcaption&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/124.png?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 5. Visualization of the algorithm process&lt;/figcaption&gt;

&lt;!-- - #### Object detection

![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/115.png?raw=true)
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 6. The architecture of object detection network&lt;/figcaption&gt;

![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/118.png?raw=true)
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 7. Synthetic training data for object detection networks&lt;/figcaption&gt;

- #### Initial pose estimation

![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/116.png?raw=true)
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 8. Convolutional Auto-encoder Network&lt;/figcaption&gt;

![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/119.png?raw=true)
![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/120.png?raw=true)
![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/117.png?raw=true)

![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/121.png?raw=true)
![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/122.png?raw=true)
![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/123.png?raw=true) --&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;5-contrast&quot;&gt;5. Contrast&lt;/h3&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/125.png?raw=true&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/126.png?raw=true&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;
&lt;/center&gt;

&lt;!-- ![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/125.png?raw=true) --&gt;

&lt;!-- ![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/126.png?raw=true) --&gt;

&lt;h5 id=&quot;aae-eccv-2018-sundermeyer-m-marton-z-c-durner-m-et-al-implicit-3d-orientation-learning-for-6d-object-detection-from-rgb-imagescproceedings-of-the-european-conference-on-computer-vision-eccv-2018-699-715&quot;&gt;AAE (ECCV 2018): Sundermeyer M, Marton Z C, Durner M, et al. Implicit 3d orientation learning for 6d object detection from rgb images[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 699-715.&lt;/h5&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;6-paper&quot;&gt;6. Paper&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;面向机器人精密操作的物体六自由度单目实时位姿估计方法研究---硕士学位论文&quot;&gt;&lt;a href=&quot;https://pan.baidu.com/s/1se1wJLHhyGKLw54SS35zAQ&quot;&gt;面向机器人精密操作的物体六自由度单目实时位姿估计方法研究 - 硕士学位论文&lt;/a&gt;&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;code-come-soon&quot;&gt;code come soon.&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;</content><author><name>yang yang</name></author><category term="project" /><category term="6D pose" /><summary type="html">&amp;lt;!–</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" /><media:content medium="image" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Patch Based Real-Time 6D Object Pose Refinement Method for Robotic Manipulation</title><link href="http://localhost:4000/pose-refinement/" rel="alternate" type="text/html" title="A Patch Based Real-Time 6D Object Pose Refinement Method for Robotic Manipulation" /><published>2020-07-01T00:00:00+08:00</published><updated>2020-07-01T00:00:00+08:00</updated><id>http://localhost:4000/pose-refinement</id><content type="html" xml:base="http://localhost:4000/pose-refinement/">&lt;!-- ![Screenshot](https://raw.githubusercontent.com/sergiokopplin/indigo/gh-pages/assets/screen-shot.png)

Example of project - Indigo Minimalist Jekyll Template - [Demo](https://sergiokopplin.github.io/indigo/). This is a simple and minimalist template for Jekyll for those who likes to eat noodles. --&gt;

&lt;h3 id=&quot;1-video-demo&quot;&gt;1. Video Demo&lt;/h3&gt;
&lt;iframe src=&quot;https://player.bilibili.com/player.html?aid=689871603&amp;amp;bvid=BV1o24y1f7NQ&amp;amp;cid=883264732&amp;amp;page=1&amp;amp;high_quality=1&amp;amp;danmaku=0&quot; allowfullscreen=&quot;allowfullscreen&quot; width=&quot;100%&quot; height=&quot;500&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; sandbox=&quot;allow-top-navigation allow-same-origin allow-forms allow-scripts&quot; high_quality=&quot;1&quot;&gt;&lt;/iframe&gt;

&lt;h4 id=&quot;-link-a-patch-based-real-time-6d-object-pose-refinement-method-for-robotic-manipulation&quot;&gt;     Link: &lt;a href=&quot;https://www.bilibili.com/video/BV1o24y1f7NQ/?share_source=copy_web&amp;amp;vd_source=926e5fb00a879a3a9c35633c5af54c69&quot;&gt;A Patch Based Real-Time 6D Object Pose Refinement Method for Robotic Manipulation&lt;/a&gt;&lt;/h4&gt;

&lt;h4 id=&quot;the-above-video-shows-the-test-results-of-our-pose-refinement-method-in-the-test-environment-and-the-open-environment-where-the-initial-refined-and-ground-truth-3d-bounding-box-are-shown-in-orange-blue-and-green-respectively&quot;&gt;  The above video shows the test results of our pose refinement method in the test environment and the open environment, where the initial, refined and Ground-Truth 3D Bounding Box are shown in orange, blue and green, respectively.&lt;/h4&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;2-image-demo&quot;&gt;2. Image Demo&lt;/h3&gt;

&lt;!-- &lt;embed src=&quot;Figure_21.pdf&quot;
 type=&quot;application/pdf&quot;  width=&quot;800px&quot; height=&quot;2400px&quot;&gt;

&lt;iframe src=&quot;Figure_21.pdf&quot;
 width=&quot;800px&quot; height=&quot;2400px&quot;&gt;&lt;/iframe&gt;

&lt;object data=&quot;Figure_21.pdf&quot;
 type=&quot;application/pdf&quot; width=&quot;800px&quot; height=&quot;2400px&quot;&gt;&lt;/object&gt; --&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yang-yang-o-o/yang-yang-o-o.github.io/main/assets/images/Figure_21.jpg&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 1. Pose refinement visualization on test data.&lt;/figcaption&gt;

&lt;h4 id=&quot;the-fig1-shows-the-pose-refinement-visualization-of-our-method-where-the-refined-and-ground-truth-3d-bounding-box-are-shown-in-blue-and-green-respectively&quot;&gt;  The Fig.1 shows the Pose refinement visualization of our method, where the refined and Ground-Truth 3D Bounding Box are shown in blue and green, respectively.&lt;/h4&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/111.png?raw=true&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;
&lt;/center&gt;

&lt;!-- ![Markdowm Image](https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/111.png?raw=true){: class=&quot;center-image&quot; } --&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 2. Robotic manipulation platform of Eye-in-Hand.&lt;/figcaption&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/Figure_12.jpg?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Fig. 3. The pose estimation results visualization of Eye-in-Hand.&lt;/figcaption&gt;

&lt;h4 id=&quot;as-shown-in-fig-3-we-fix-the-relative-pose-between-the-calibration-board-and-the-object-to-evaluate-the-pose-refinement-accuracy-under-various-camera-view-points&quot;&gt;  As shown in Fig. 3, we fix the relative pose between the calibration board and the object to evaluate the pose refinement accuracy under various camera view-points.&lt;/h4&gt;

&lt;hr /&gt;
&lt;!-- What has inside?

- Gulp
- BrowserSync
- Stylus
- SVG
- No JS
- [98/100](https://developers.google.com/speed/pagespeed/insights/?url=http%3A%2F%2Fsergiokopplin.github.io%2Findigo%2F) --&gt;
&lt;h3 id=&quot;3-key-inside&quot;&gt;3. Key-inside&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Siamese neural network&lt;/li&gt;
  &lt;li&gt;Image patch matching&lt;/li&gt;
  &lt;li&gt;PnP algorithm&lt;/li&gt;
  &lt;li&gt;Opencv&lt;/li&gt;
  &lt;li&gt;Pinhole camera model&lt;/li&gt;
  &lt;li&gt;Rigid Object 6D pose description&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;4-method&quot;&gt;4. Method&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/127.jpg?raw=true&quot; alt=&quot;Markdowm Image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;5-contrast&quot;&gt;5. Contrast&lt;/h3&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/128.png?raw=true&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/images/129.png?raw=true&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;
&lt;/center&gt;

&lt;h5 id=&quot;yolo6d-cvpr-2018-b-tekin-s-n-sinha-and-p-fua-real-time-seamless-single-shot-6d-object-pose-prediction-in-proceedings-of-the-ieee-conference-on-computer-vision-and-pattern-recognition-2018-pp-292301&quot;&gt;YOLO6D (CVPR 2018): B. Tekin, S. N. Sinha, and P. Fua, “Real-time seamless single shot 6d object pose prediction,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 292–301&lt;/h5&gt;

&lt;h5 id=&quot;cullnet-iccvw-2019-k-gupta-l-petersson-and-r-hartley-cullnet-calibrated-and-pose-aware-confidence-scores-for-object-pose-estimation-in-proceedings-of-the-ieeecvf-international-conference-on-computer-vision-workshops-2019-pp-00&quot;&gt;CullNet (ICCVW 2019): K. Gupta, L. Petersson, and R. Hartley, “Cullnet: Calibrated and pose aware confidence scores for object pose estimation,” in Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019, pp. 0–0&lt;/h5&gt;

&lt;h5 id=&quot;pvnet-cvpr-2019-s-peng-y-liu-q-huang-x-zhou-and-h-bao-pvnet-pixel-wise-voting-network-for-6dof-pose-estimation-in-proceedings-of-the-ieeecvf-conference-on-computer-vision-and-pattern-recognition-2019-pp-45614570&quot;&gt;PVNet (CVPR 2019): S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, “Pvnet: Pixel-wise voting network for 6dof pose estimation,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561–4570&lt;/h5&gt;

&lt;hr /&gt;
&lt;!-- [Check it out](https://sergiokopplin.github.io/indigo/) here.
If you need some help, just [tell me](https://github.com/sergiokopplin/indigo/issues). --&gt;
&lt;h3 id=&quot;6-paper&quot;&gt;6. Paper&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;a-patch-based-real-time-6d-object-pose-refinement-method-for-robotic-manipulation-password-yang&quot;&gt;&lt;a href=&quot;https://pan.baidu.com/s/1se1wJLHhyGKLw54SS35zAQ&quot;&gt;A Patch Based Real-Time 6D Object Pose Refinement Method for Robotic Manipulation&lt;/a&gt;, password: yang&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;code-come-soon&quot;&gt;code come soon.&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;</content><author><name>yang yang</name></author><category term="project" /><category term="6D pose" /><summary type="html">&amp;lt;!–</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" /><media:content medium="image" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">6D object pose estimation based on YOLO6d</title><link href="http://localhost:4000/Yolo6d/" rel="alternate" type="text/html" title="6D object pose estimation based on YOLO6d" /><published>2020-03-01T00:00:00+08:00</published><updated>2020-03-01T00:00:00+08:00</updated><id>http://localhost:4000/Yolo6d</id><content type="html" xml:base="http://localhost:4000/Yolo6d/">&lt;!-- &lt;p align=&quot;center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/SHocQxS0PCc&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt; --&gt;
&lt;h3 id=&quot;1-video-demo&quot;&gt;1. Video Demo&lt;/h3&gt;
&lt;iframe src=&quot;//player.bilibili.com/player.html?bvid=BV1He4y1x7du&amp;amp;page=1&amp;amp;high_quality=1&amp;amp;danmaku=0&quot; allowfullscreen=&quot;allowfullscreen&quot; width=&quot;100%&quot; height=&quot;500&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; sandbox=&quot;allow-top-navigation allow-same-origin allow-forms allow-scripts&quot;&gt;&lt;/iframe&gt;

&lt;h4 id=&quot;the-video-above-demonstrates-yolo6d-based-6d-pose-estimation-where-the-projection-of-the-3d-bbox-is-shown-in-green&quot;&gt;  The video above demonstrates yolo6d-based 6D pose estimation, where the projection of the 3D bbox is shown in green.&lt;/h4&gt;
&lt;!-- Example of project - Indigo Minimalist Jekyll Template - [Demo](https://sergiokopplin.github.io/indigo/). This is a simple and minimalist template for Jekyll for those who likes to eat noodles. --&gt;

&lt;hr /&gt;

&lt;!-- ![Screenshot](https://raw.githubusercontent.com/sergiokopplin/indigo/gh-pages/assets/screen-shot.png)

--- --&gt;

&lt;h3 id=&quot;2-key-inside&quot;&gt;2. Key-inside&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;YOLO&lt;/li&gt;
  &lt;li&gt;EPnP algorithm&lt;/li&gt;
  &lt;li&gt;Opencv&lt;/li&gt;
  &lt;li&gt;Aruco&lt;/li&gt;
  &lt;li&gt;Pinhole camera model&lt;/li&gt;
  &lt;li&gt;Rigid Object 6D pose description&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;3-reference&quot;&gt;3. Reference&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;yolo6d-paper-source-code&quot;&gt;Yolo6D: &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2018/papers/Tekin_Real-Time_Seamless_Single_CVPR_2018_paper.pdf&quot;&gt;paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/singleshotpose&quot;&gt;source code&lt;/a&gt;&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- [Check it out](https://sergiokopplin.github.io/indigo/) here.
If you need some help, just [tell me](https://github.com/sergiokopplin/indigo/issues). --&gt;

&lt;hr /&gt;</content><author><name>yang yang</name></author><category term="project" /><category term="6D pose" /><summary type="html">1. Video Demo</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" /><media:content medium="image" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>