---
title: "Fcos3d论文阅读"
layout: post
date: 2022-11-03 22:48
# image: /assets/images/markdown.jpg
# headerImage: false
tag:
- 3D检测
category: blog
# author: jamesfoster
# description: Markdown summary with different options
---

### 1、contribution

- 扩展Fcos，提出了一个通用单目3D目标检测框架Fcos3d

### 2、网络结构

<center>
<img src="https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/fcos3d/Fcos3d_network.png?raw=true" width = "90%" height = "90%"/>
</center>

### 3、训练和推理

#### - 前向推理结果output张量

网络输入HxWxC的图像，经过一个backbone和FPN，输出5个level的feature map，分别为【8、16、32、64、128】倍降采样，5个level的feature map 先后经过同一个Head(共享权重)，最终输出三个张量：分别为HxWxC（大类加细分类）、HxWx1（Centerness）、HxWx11（Offset，Depth，Size，Rotation，Dir.Class，Velocity），即feature map上每个点只产生一个预测。在共享权重的Head中，Centerness和box分支共享同一个x4的卷积层，然后各自接自己的小head输出。Class分支单独使用一个x4卷积然后接一个小head输出。

在不同的特征层之间共享HEAD（共享权重），为了适应不同的feature level需要回归的不同大小范围，在处理模型输出时，不使用标准的exp(x)，而是使用带有可训练标量si的exp(si*x)来为feature level i自动调整指数函数的基数。

#### - 如何根据标签生成target张量

如果feature map 上的一个位置P(x,y)映射回输入图像为位置p(s/2+xs,s/2+ys)，通过位置P(x,y)的预测结果（C+1+11）可以解析出预测3D框，其中Offset是预测的3D框的中心3D点在图像上的投影2d点相对于位置p的Δx和Δy，而Depth是预测的3D框的中心3D点在相机坐标系下的深度，根据位置p(s/2+xs,s/2+ys)、Offset和Depth就可以求出预测的3D框的3D中心点在相机坐标系下的坐标值，而预测3D框的长宽高从Size解析得到，朝向从Rotation和Dir.Class解析得到。

feature map 上的每一个位置P(x,y)都解析出预测的3Dbox后，将target 3Dbox的八个角点都投影回输入图像上，然后求八个投影点的最小外接矩形，用这个最小外接矩形来计算fcos中的L*，T*，R*，B*，然后根据fcos中的 max(L*，t*，r*，b*) > mi 和 max(L*，t*，r*，b*) < mi-1 来将每个target 3Dbox分配到某一个feature level上，因此实际上在fcos3d方法中，没有用到2D的标签框。

**确定正负样本：** target 3Dbox 分配完成后，每个feature level分配到一些target 3Dbox，对于每个feature level，如果其上的某个位置P_o(x,y)解析出的3Dbox的3D中心点与任何一个target 3Dbox的3D中心点之间的距离都大于阈值（论文中取1.5m），那么这个位置P_o(x,y)被作为负样本。如果某些位置P_i(x,y)解析出的3Dbox的3D中心点与其中一个target 3Dbox的3D中心点之间的距离小于阈值，那么这些位置P_i(x,y)将被作为正样本。

如果一个位置P(x,y)预测的3D中心点的投影点落在了多个target 3Dbox的2d投影框内，就将中心点最接近3D中心点投影点的2d投影框对应的target 3Dbox分配给位置P(x,y)来预测。

最终按照上述的步骤依次处理完每个feature level，每个target 3Dbox都被分配给一个位置P(x,y)来预测，负责预测target 3Dbox的位置P(x,y)被作为正样本，其他位置被作为负样本。

- 类别的GT可以直接根据target 3Dbox的类别来设置
- Centerness的GT：将target 3Dbox的3D中心点投影回输入图像，然后以投影点为中心构建二维高斯分布，Centerness的GT值就是负责预测这个target 3Dbox的位置P(x,y)的投影位置p(s/2+xs,s/2+ys)处的高斯分布值。
- 回归值的GT：将target 3Dbox的3D中心点投影回输入图像，相对于位置p(s/2+xs,s/2+ys)可得到Δ，然后除以宽高后映射到feature map上作为Offset的GT值；对于Depth的GT值dp，由target 3Dbox的3D中心点的z坐标dg乘以s和常数c得到，fx和fy分别相机x和y方向的焦距，dp的物理意义是物体真实宽高和归一化平面内所占像素的比值，因为 x_c/u_x = dg/1, x_c/(fx*u_x) = dg/fx, 真实宽/归一化平面内x方向像素个数 = dg/fx，x_c和u_x分别为真实的宽和归一化平面内的宽。

<center>
<img src="https://github.com/yang-yang-o-o/yang-yang-o-o.github.io/blob/main/assets/fcos3d/Fcos3d_depth.png?raw=true" width = "90%" height = "90%"/>
</center>

对于Size、Rotation、Dir.Class的GT值，可直接从target 3Dbox获取。

#### - 如何根据output张量和target张量来计算loss

对于正样本计算类别损失、Centerness损失和回归损失；对于负样本只计算类别损失

- **类别损失采用focal loss，属性分类损失采用softmax classification loss**

- **Centerness损失采用 BCE loss**

- **回归损失使用smooth L1 loss，其中Dir.Class损失使用softmax classification loss**

#### - 推理时如何从output张量中解析出最终的检测结果

- 预测的Centerness乘到类别概率上作为置信度做NMS
- 根据预测的Offset（Δx和Δy）结合位置p(s/2+xs,s/2+ys)得到x和y的图像坐标，结合内参和预测的Depth得到预测3Dbox的3D中心点（相机坐标系下），根据预测的Size得到3Dbox的长宽高，根据预测的Rotation和Dir.Class得到3Dbox的朝向角

### 参考

- https://arxiv.org/abs/1904.01355
- https://github.com/open-mmlab/mmdetection3d
