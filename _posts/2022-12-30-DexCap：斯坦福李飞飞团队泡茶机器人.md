---
title: "DexCap：斯坦福李飞飞团队泡茶机器人"
layout: post
date: 2022-12-30 22:48
# image: /assets/images/markdown.jpg
# headerImage: false
tag:
- 机器人学习
category: blog
# author: jamesfoster
# description: Markdown summary with different options
---

<!-- ![Markdowm Image](/assets\DexCap\image_7.png) -->
![Markdowm Image](https://raw.githubusercontent.com/yang-yang-o-o/yang-yang-o-o.github.io/main/assets\DexCap\image_7.png)

&emsp;[**项目地址**](https://dex-cap.github.io/) &emsp;&emsp; [**论文地址**](https://dex-cap.github.io/assets/DexCap_paper.pdf)

## 1、Contribution

- DEXCAP: 一种新型便携式人体手部动作捕捉系统，能够实时跟踪手腕和手指的运动，实现灵巧的操作任务。
- DEXIL: 一个模仿学习框架，利用手部动作捕捉数据和点云观察，直接学习灵巧的操作技能。
- Human-in-the-Loop Correction: 采用DEXCAP的校正机制，显著提高机器人在复杂任务中的性能。

<!-- ![Markdowm Image](/assets\DexCap\image_1.png) -->
![Markdowm Image](https://raw.githubusercontent.com/yang-yang-o-o/yang-yang-o-o.github.io/main/assets\DexCap\image_1.png)

## 2、DEXCAP

<!-- ![Markdowm Image](/assets\DexCap\image_3.png) -->
![Markdowm Image](https://raw.githubusercontent.com/yang-yang-o-o/yang-yang-o-o.github.io/main/assets\DexCap\image_3.png)

为了捕获适合训练灵巧机器人策略的细粒度手部运动数据，DEXCAP设计了四个关键目标：

- **追踪手指运动**：使用EMF(electromagnetic field)手套

- **跟踪6DoF手腕姿态**：开发了一种基于SLAM算法的6自由度手腕跟踪系统，如Fig. 2(c)所示。该系统使用Intel Realsense T265摄像头，安装在每只手套的背面。它将来自两个鱼眼摄像头的图像和IMU传感器信号结合起来，使用SLAM算法构建环境地图，从而实现对手腕6自由度姿势的一致跟踪。

- **记录三维观测和校准**：在数据采集开始时，所有跟踪摄像机都放置在机架槽中，保证了摄像机帧之间的恒定变换。然后，我们从架子上取下跟踪摄像头，把它们插到每只手套上的摄像头槽里。这样，我们就可以很容易地将手部姿态跟踪结果通过恒定的初始变换转化为胸部相机的观察帧。

- **系统的可移植性**：我们将为构建者开放代码和指导视频，以及一系列硬件选项。DEXCAP的总成本控制在4000美元的预算之内。

<!-- ![Markdowm Image](/assets\DexCap\image_4.png) -->
![Markdowm Image](https://raw.githubusercontent.com/yang-yang-o-o/yang-yang-o-o.github.io/main/assets\DexCap\image_4.png)

对于机器人设置，只使用激光雷达相机，不需要手腕相机。

## 3、DEXIL & Human-in-the-Loop Correction

<!-- ![Markdowm Image](/assets\DexCap\image_2.png) -->
![Markdowm Image](https://raw.githubusercontent.com/yang-yang-o-o/yang-yang-o-o.github.io/main/assets\DexCap\image_2.png)

### 3.1、Data re-targeting

- 将DEXCAP数据重新定位到机器人实施体的动作和观察空间

- 部署了一种IK算法，确保DEXCAP数据中机器人指尖和人类指尖之间的对齐，计算中排除小指

- 所有来自动作捕捉数据的RGB-D帧被处理成与机器人空间对齐的点云，并排除与任务无关的元素，如桌面点。这个精炼的点云数据因此成为机器人策略π的观测输入。

### 3.2、Point cloud-based Diffusion Policy

<!-- ![Markdowm Image](/assets\DexCap\image_5.png) -->
![Markdowm Image](https://raw.githubusercontent.com/yang-yang-o-o/yang-yang-o-o.github.io/main/assets\DexCap\image_5.png)

- 使用重新定位的数据训练基于点云的扩散策略[[73]](#73)[[30]](#30)
- Policy model π 使用 Diffusion Policy[[13]](#13)[[74]](#74)学习得到
    - 模型输入为 $o_t$（场景点云和颜色信息$R^{Kx6}$）和 $s_t$（机器人的状态，$s_t=(p_t,J_t)$，机械臂末端位姿 $p_t=[R_t,T_t]$，手指关节位姿 $J_t$）  
    - 输出生成的机械臂运动 $a_{[t:t+d]}$，即运动轨迹($a_t$,$a_{t+1}$,...,$a_{t+d}$)，而 $a_t=s_{t+1}$

### 3.3、Human-in-the-loop correction

<!-- ![Markdowm Image](/assets\DexCap\image_6.png) -->
![Markdowm Image](https://raw.githubusercontent.com/yang-yang-o-o/yang-yang-o-o.github.io/main/assets\DexCap\image_6.png)

- 可选的纠正机制，旨在解决策略执行期间出现的意外行为

- 当需要大的位置变化时，按下脚踏板将系统切换到远程操作模式。DEXCAP现在忽略策略的推出，并将人类手腕delta直接应用于机器人手腕姿势。机器人的指尖现在直接跟随人类的指尖。

- 用户也可以在纠正机器人错误后，再次踩下脚踏板，切换回剩余模式。

- 修正后的动作和观察结果存储在一个新的数据集D'中。训练数据以等概率从D'和原始数据集D中采样，以微调策略模型，类似于IWR[[46]](#46)。

## 4、Experiments

**.....**

## 5、Conclusion

在未来的工作中，我们的目标是利用互联网规模的人手运动数据来预训练策略。像[[78]](#78)这样的分层策略架构也可以在这种情况下显示效果。(_In future works, we
aim to leverage the internet-scale human hand motion data to
pretrain the policy. Hierarchical policy architectures like [[78]](#78) could also show effects in such scenarios._)

## 6、Reference

<div id="13"></div>
- [13] [Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023.](https://arxiv.org/pdf/2303.04137)

<div id="30"></div>
- [30] [Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020.](https://arxiv.org/pdf/2006.11239)

<div id="46"></div>
- [46] [Ajay Mandlekar, Danfei Xu, Roberto Mart´ ın-Mart´ ın, Yuke Zhu, Li Fei-Fei, and Silvio Savarese. Human-in-the-loop imitation learning using remote teleoperation. arXiv preprint arXiv:2012.06733, 2020.](https://arxiv.org/pdf/2012.06733)

<div id="73"></div>
- [73] [Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256–2265. PMLR, 2015.](https://arxiv.org/pdf/1503.03585)

<div id="74"></div>
- [74] [Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.](https://arxiv.org/abs/2010.02502)

<div id="78"></div>
- [78] [Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anandkumar. Mimicplay: Long-horizon imitation learning by watching human play. arXiv preprint arXiv:2302.12422, 2023.](https://arxiv.org/pdf/2302.12422)

## 参考

- https://dex-cap.github.io/
- https://dex-cap.github.io/assets/DexCap_paper.pdf
- https://blog.csdn.net/v_JULY_v/article/details/139410045
