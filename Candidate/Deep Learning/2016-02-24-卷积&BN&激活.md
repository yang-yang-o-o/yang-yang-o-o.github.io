---
title: "卷积&BN&激活"
layout: post
# date: 2016-02-24 22:48
# image: /assets/images/markdown.jpg
# headerImage: false
tag:
- Deep Learning
category: blog
# author: jamesfoster
# description: Markdown summary with different options
---

### 1. 卷积

QA：为什么卷积核大小是奇数
卷积核大小通常选择奇数是为了保持对称性和中心对齐的特性。卷积操作是将卷积核与输入图像逐个位置进行乘积并求和的过程。在这个过程中，卷积核的中心元素被放置在当前位置的像素上，然后通过对称性将其余元素分布在中心周围。

如果卷积核的大小是偶数，那么它就没有中心元素，无法准确地定位到输入图像的某个像素。这可能导致计算结果的偏移和不对称性。而选择奇数大小的卷积核，可以确保有一个中心元素，使得卷积操作更加精确和对称。

此外，使用奇数大小的卷积核还有一个好处是在进行步幅为1的卷积操作时，可以保持输入和输出的大小一致。如果卷积核大小是偶数，那么在进行步幅为1的卷积操作时，输入和输出的大小就无法保持一致，需要采取额外的处理方式，如填充(padding)操作。

因此，为了保持对称性、中心对齐和输入输出一致性，通常选择奇数大小的卷积核。但在某些特殊情况下，偶数大小的卷积核也可以使用，并采取相应的调整方法来处理中心对齐和输入输出的一致性。

当选择卷积核大小时，通常选择奇数的原因有以下几点：

1. 对称性和中心对齐：奇数大小的卷积核能够保持对称性，并且有一个清晰的中心元素。这有助于在卷积操作中保持对称性，确保卷积核能够准确地定位到输入图像的像素。这样做可以使卷积操作更加精确和对称。

2. 权重共享：卷积神经网络（CNN）利用权重共享的特性，即在输入数据的不同空间位置上使用相同的权重（参数）集合。权重共享使得CNN能够学习具有空间不变性的特征。奇数大小的卷积核简化了权重共享的过程，因为卷积核有一个明确的中心位置，更容易有效地共享相同的权重。

3. 平移不变性：奇数大小的卷积核具有自然的平移不变性。平移不变性意味着学习到的特征可以无视其在输入图像中的位置而检测相同的模式。使用奇数大小的卷积核可以保持输出特征图与输入的相同空间尺寸，有助于保持平移不变性。

4. 填充和步幅：使用奇数大小的卷积核可以更容易地处理填充和步幅的配置。填充通常用于确保输出特征图与输入具有相同的空间尺寸。奇数大小的卷积核可以在输入的两侧均匀地添加填充，保持中心对齐。同样，当应用步幅为1以外的值时，奇数大小的卷积核可以很容易地保持中心对齐，确保卷积核对输入进行对称移动。

5. 计算效率：奇数大小的卷积核在计算复杂度和内存使用方面具有优势。使用奇数大小可以减少参数的数量，因为不需要处理额外的中心元素（当卷积核大小为偶数时需要处理）。参数的减少可以提高训练和推断的效率。

虽然奇数大小的卷积核在卷积操作中常见，但在某些情况下，可能会使用偶数大小的卷积核。例如，在某些架构中（如Inception模块），会使用奇数和偶数大小的卷积核组合，以捕获不同尺度的特征。此外，在某些特定的场景中，偶数大小的卷积核可能更有用，例如需要处理非对称或偏离中心的特征时。

总而言之，选择奇数大小的卷积核主要基于对称性、中心对齐、权重共享、平移不变性以及填充和步幅配置的便利性。这些特性有助于提高卷积神经网络在各种计算机视觉任务中的效果和效率。
### 2. Batch Nomalization


### 3. 激活函数
